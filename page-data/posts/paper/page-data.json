{
    "componentChunkName": "component---gatsby-theme-academic-src-templates-post-post-jsx",
    "path": "/posts/paper",
    "result": {"data":{"mdx":{"timeToRead":1,"tableOfContents":{"items":[{"url":"#world-models--video-reasoning","title":"World Models & Video Reasoning","items":[{"url":"#video-position-encoding--temporalspacial-attention","title":"Video Position Encoding & Temporal/Spacial Attention"},{"url":"#datasets","title":"Datasets"}]},{"url":"#video-generation","title":"Video Generation","items":[{"url":"#benchmark","title":"Benchmark"}]},{"url":"#long-context-llm","title":"Long Context LLM"},{"url":"#diffusion-models","title":"Diffusion Models"},{"url":"#mlsys","title":"MLSys"}]},"frontmatter":{"cover":null,"title":"Paper List","date":"2024-06-25T00:00:00.000Z","tags":["paper"],"path":"posts/paper","excerpt":"How I hope I won't forget the papers I have read.","links":[],"commit":0,"type":"posts"},"fileAbsolutePath":"/home/runner/work/ssyuw.github.io/ssyuw.github.io/example/content/posts/paper/index.md","fields":{"slug":{"html":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Paper List\",\n  \"tags\": [\"paper\"],\n  \"date\": \"2024-06-25T00:00:00.000Z\",\n  \"path\": \"posts/paper\",\n  \"excerpt\": \"How I hope I won't forget the papers I have read.\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h2\", {\n    \"id\": \"world-models--video-reasoning\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#world-models--video-reasoning\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"World Models & Video Reasoning\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://world-model.maitrix.org/assets/pandora.pdf\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Pandora: Towards General World Model with Natural Language Actions and Video States\"), \", 2024 | \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/maitrix-org/Pandora?tab=readme-ov-file\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Code\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://haofei.vip/VoT/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Video-of-Thought: Step-by-Step Video Reasoning from Perception to Cognition\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Insightful and comprehensive. The conecept of spatial-temporal scene graph (STSG) is something new to me.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/html/2407.08693v1\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Robotic Control via Embodied Chain-of-Thought Reasoning\"))), mdx(\"h3\", {\n    \"id\": \"video-position-encoding--temporalspacial-attention\"\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#video-position-encoding--temporalspacial-attention\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Video Position Encoding & Temporal/Spacial Attention\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2403.13298\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Rotary Position Embedding for Vision Transformer\"), \", 2024\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The application of RoPE in video transformers. Compared to the S/T Attention, it's something different.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2201.05991\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Video Transformers: A Survey\"), \", 2022\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://link.springer.com/article/10.1007/s10489-023-04756-5\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Space or time for video classification transformers\"), \", 2023\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The concept of Space Attention and Temporal Attention is interesting. \")))), mdx(\"h3\", {\n    \"id\": \"datasets\"\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#datasets\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Datasets\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2111.10337\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"(hdvila) Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions\\n\"), \" | \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/microsoft/XPretrain/tree/main/hd-vila\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Code\"), \", 2021\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2405.03690\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs\"), \" | \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/mbzuai-oryx/CVRR-Evaluation-Suite/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Code\"), \", 2024\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://sharegpt4video.github.io/\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"ShareGPT4Video:\\nImproving Video Understanding and Generation with Better Captions\"), \", 2024\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The differencial data annotation method is interesting, we want to have fine-grained annotations for the video data.\")))), mdx(\"h2\", {\n    \"id\": \"video-generation\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#video-generation\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Video Generation\"), mdx(\"h3\", {\n    \"id\": \"benchmark\"\n  }, mdx(\"a\", {\n    parentName: \"h3\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#benchmark\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Benchmark\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2311.17982\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"VBench: Comprehensive Benchmark Suite for Video Generative Models\"), \" | \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://github.com/Vchitect/VBench\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Code\"), \", 2023 (CVPR 2024)\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Comprehensive benchmark for video generation models. \")))), mdx(\"h2\", {\n    \"id\": \"long-context-llm\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#long-context-llm\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Long Context LLM\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2104.09864\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"), \", 2020\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2405.14591\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Base of RoPE Bounds Context Length\"), \", 2024\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Explore the influence of the base of RoPE on the context length of the model.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2406.09897\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"3D-RPE: Enhancing Long-Context Modeling Through 3D Rotary Position Encoding\"), \", 2024\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Creative idea to introduce a new dimension to the RoPE, which is called a chunk. Attention of attention.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2404.12096\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"LongEmbed: Extending Embedding Models for Long Context Retrieval\"), \", 2024 | \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/dwzhu-pku/LongEmbed\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Code\")), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A comprehensive experimental study on different methods to extend context window (e.g. Parallel Context Window, NTK, self-extend, Grouped Position & Reccurent Position, etc.). Also introduce a benchmark called LongEmbed.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2405.14722\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"CAPE: Context-Adaptive Positional Encoding for Length Extrapolation\"), \", 2024\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Exploring on using NNs to further enhance the additive PE methods.\")))), mdx(\"h2\", {\n    \"id\": \"diffusion-models\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#diffusion-models\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"Diffusion Models\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/showlab/Awesome-Video-Diffusion?tab=readme-ov-file\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"https://github.com/showlab/Awesome-Video-Diffusion?tab=readme-ov-file\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/2406.08929\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Step-by-Step Diffusion: An Elementary Tutorial\"), \", 2024\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2208.11970\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"Understanding Diffusion Models: A Unified Perspective\"), \", 2022\"))), mdx(\"h2\", {\n    \"id\": \"mlsys\"\n  }, mdx(\"a\", {\n    parentName: \"h2\",\n    \"aria-hidden\": \"true\",\n    \"tabIndex\": -1,\n    \"href\": \"#mlsys\"\n  }, mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"icon icon-link\"\n  })), \"MLSys\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"vLLM (PagedAttention): \", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=Oq2SN7uutbQ\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"https://www.youtube.com/watch?v=Oq2SN7uutbQ\"), \" (CN)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.youtube.com/watch?v=5ZlavKF_98U\",\n    \"target\": \"_blank\",\n    \"rel\": \"nofollow\"\n  }, \"https://www.youtube.com/watch?v=5ZlavKF_98U\"), \" (EN)\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  })));\n}\n;\nMDXContent.isMDXComponent = true;","htmlEncrypted":"","nonce":""}}}},"pageContext":{"fileAbsolutePath":"/home/runner/work/ssyuw.github.io/ssyuw.github.io/example/content/posts/paper/index.md","postPath":"posts/paper","translations":[{"hreflang":"en","path":"/posts/paper"}]}},
    "staticQueryHashes": ["1552981879","2213578703","4097791827"]}